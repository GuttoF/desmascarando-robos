{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desmascarando Robôs\n",
    "### CRISP-DM Cycle 1\n",
    "---\n",
    "\n",
    "Imagine um mercado online, um palco digital onde diversos leilões se desenrolam a cada segundo. Neste ambiente, participantes do mundo inteiram lançam seus lances em busca de objetos desejados, desde joias até equipamentos tecnológicos. No entanto, nem todos os jogadores neste campo são humanos; alguns são robôs programados para manipular os resultados dos leilões.\n",
    "\n",
    "Seu desafio é se aprofundar nesses dados, explorar as camadas de atividade nos leilões e conseguir construir um modelo que saiba muito bem diferenciar humanos de robôs.\n",
    "\n",
    "> Disclaimer: This is a fictional bussiness cas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. PREPARATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings imports\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load .env file\n",
    "env_path = \"../.env\"\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "\n",
    "# Seed\n",
    "seed = int(os.getenv(\"SEED\"))\n",
    "\n",
    "# Add path\n",
    "path = os.getenv(\"HOMEPATH\")\n",
    "\n",
    "# Add path to sys.path\n",
    "sys.path.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    GradientBoostingClassifier,\n",
    "    AdaBoostClassifier,\n",
    "    ExtraTreesClassifier,\n",
    ")\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train e Test**\n",
    "\n",
    "- **id_participante**: Identificador único do participante\n",
    "- **conta_pagamento**: Conta de pagamento associada ao participante (com o valor ocultado) # Não será utilizada\n",
    "- **endereco**: Endereço postal do participante # Não será utilizada\n",
    "- **resultado**: A variável alvo que identifica se o participante é um robô ou um humano. (Robô = 1 e Humano = 0). (*target*)\n",
    "\n",
    "- **Robôs Confirmados**: Participantes com provas claras de atividades fraudulentas, resultando em banimento da plataforma. São rotulados como robôs no conjunto de dados (resultado = 1).\n",
    "\n",
    "- **Robôs Suspeitos**: Participantes com atividades atípicas ou estatísticas que superam a média, mas sem provas definitivas de fraude. A classificação deles como robôs é incerta.\n",
    "\n",
    "**Lances**\n",
    "\n",
    "- **id_lance**: Identificador único do lance\n",
    "- **id_participante**: Identificador único do participante\n",
    "- **leilao**: Identificador único do leilão \n",
    "- **mercadoria**: A categoria da mercadoria leiloada\n",
    "- **dispositivo**: O dispositivo utilizado pelo visitante\n",
    "- **tempo**: O tempo que o lance foi feito\n",
    "- **pais**: O país que o IP pertence\n",
    "- **ip**: O IP do participante\n",
    "- **url**: A URL de onde o participante foi referido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "253209\n"
     ]
    }
   ],
   "source": [
    "X_train = pd.read_feather(path + \"/data/processed/X_train.feather\")\n",
    "X_test = pd.read_feather(path + \"/data/processed/X_test.feather\")\n",
    "X_val = pd.read_feather(path + \"/data/processed/X_val.feather\")\n",
    "\n",
    "\n",
    "y_train = pd.read_pickle(path + \"/data/processed/y_train.pkl\")\n",
    "y_test = pd.read_pickle(path + \"/data/processed/y_test.pkl\")\n",
    "y_val = pd.read_pickle(path + \"/data/processed/y_val.pkl\")\n",
    "\n",
    "print(len(X_train.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from helper.classes.Pipeline import MLPipeline\n",
    "\n",
    "\n",
    "feature_transformations = {\n",
    "    \"log\": [\n",
    "        \"contagem_participante\",\n",
    "        \"contagem_leilao\",\n",
    "        \"contagem_conta_pagamento\",\n",
    "        \"frequencia_dispositivo\",\n",
    "    ],\n",
    "    \"one_hot\": [\n",
    "        \"dispositivo\",\n",
    "        \"leilao\",\n",
    "        \"periodo_dia\",\n",
    "        \"mercadoria\",\n",
    "        \"conta_pagamento\",\n",
    "    ],\n",
    "    \"ordinal\": [\"ip_classe\"],\n",
    "    \"hashing\": [\"pais\", \"url\", \"endereco\"],\n",
    "    \"min_max_scaler\": [\n",
    "        \"hora_sin\",\n",
    "        \"hora_cos\",\n",
    "        \"minuto_sin\",\n",
    "        \"minuto_cos\",\n",
    "        \"segundo_sin\",\n",
    "        \"segundo_cos\",\n",
    "    ],\n",
    "    \"robust_scaler\": [\"hora\", \"minuto\", \"segundo\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proportion = float(len(y_train[y_train == 0])) / len(y_train[y_train == 1])\n",
    "class_weights = {0: 0.87, 1: 0.13}\n",
    "\n",
    "models = [\n",
    "    # KNeighborsClassifier(n_neighbors=5),\n",
    "    # LogisticRegression(class_weight=class_weights, random_state=seed),\n",
    "    # DecisionTreeClassifier(class_weight=class_weights, random_state=seed),\n",
    "    RandomForestClassifier(class_weight=class_weights, random_state=seed),\n",
    "    # AdaBoostClassifier(random_state=seed),\n",
    "    # GradientBoostingClassifier(random_state=seed),\n",
    "    # ExtraTreesClassifier(class_weight=class_weights, random_state=seed),\n",
    "    # XGBClassifier(\n",
    "    #    scale_pos_weight=proportion,\n",
    "    #    objective=\"binary:logistic\",\n",
    "    #    eval_metric=\"logloss\",\n",
    "    #    random_state=seed,\n",
    "    # ),\n",
    "    # LGBMClassifier(\n",
    "    #    is_unbalance=True,\n",
    "    #    objective=\"binary\",\n",
    "    #    metric=\"binary_logloss\",\n",
    "    #    random_state=seed,\n",
    "    # ),\n",
    "    # CatBoostClassifier(scale_pos_weight=proportion, random_state=seed),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_train.index))\n",
    "print(len(y_train.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler, MinMaxScaler, RobustScaler, OneHotEncoder, FunctionTransformer\n",
    ")\n",
    "from category_encoders import OrdinalEncoder, HashingEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def test_transformations(X, feature_transformations):\n",
    "    transformers = {\n",
    "        'log': FunctionTransformer(np.log1p, validate=True),\n",
    "        'one_hot': OneHotEncoder(sparse_output=False, handle_unknown='ignore'),\n",
    "        'hashing': HashingEncoder(),\n",
    "        'ordinal': OrdinalEncoder(),\n",
    "        'standard_scaler': StandardScaler(),\n",
    "        'min_max_scaler': MinMaxScaler(),\n",
    "        'robust_scaler': RobustScaler()\n",
    "    }\n",
    "    \n",
    "    for transformer_name, features in feature_transformations.items():\n",
    "        print(f\"Testing {transformer_name} transformation on features: {features}\")\n",
    "        transformer = transformers[transformer_name]\n",
    "        try:\n",
    "            transformed = transformer.fit_transform(X[features])\n",
    "            print(f\"Transformation {transformer_name} succeeded. Output shape: {transformed.shape}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Transformation {transformer_name} failed with error: {e}\")\n",
    "            \n",
    "test_transformations(X_train, feature_transformations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = MLPipeline(feature_transformations, models)\n",
    "resultados = pipeline.run(X_train, y_train, X_val, y_val)\n",
    "resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline = MLPipeline(X_train, y_train, models)\n",
    "# pipeline.build_pipeline(\n",
    "#    log_list=log_list,\n",
    "#    ohe_list=ohe_list,\n",
    "#    ordinal_list=ordinal_list,\n",
    "#    robust_scaler_list=robust_scaler_list,\n",
    "#    min_max_scaler_list=min_max_scaler_list,\n",
    "#    standard_scaler_list=standard_scaler_list,\n",
    "# )\n",
    "\n",
    "# Treinar e avaliar modelos\n",
    "# evaluation_results = pipeline.train_and_evaluate_models(X_val, y_val)\n",
    "# evaluation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para Cross-Validation\n",
    "# pipeline_cv = MLPipelineCV(X_train, y_train, models)\n",
    "# pipeline_cv.build_pipeline(log_list, ohe_list, catboost_list, robust_scaler_list, min_max_scaler_list, standard_scaler_list)\n",
    "\n",
    "# Treinar e avaliar modelos com Cross-Validation\n",
    "# cv_results = pipeline_cv.train_and_evaluate_cv()\n",
    "# cv_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
